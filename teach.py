# -*- coding: utf-8 -*-
import os

from langchain_chroma import Chroma
from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_classic.retrievers.document_compressors import LLMChainExtractor
# import docx
from langchain_community.document_loaders import DirectoryLoader, TextLoader, Docx2txtLoader
from langchain_community.chat_models import ChatZhipuAI
from langchain_community.document_loaders import DirectoryLoader
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import networkx as nx
import matplotlib

matplotlib.use('TkAgg')  # æˆ– 'Qt5Agg'ï¼Œå¿…é¡»åœ¨ import pyplot ä¹‹å‰

import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei']  # æ”¯æŒä¸­æ–‡
plt.rcParams['axes.unicode_minus'] = False
from typing import List, Tuple
import re

from langchain_text_splitters import RecursiveCharacterTextSplitter

# =======================
# 1. é…ç½®æ¨¡å‹ä¸è·¯å¾„
# =======================

# API Key å’Œæ¨¡å‹é…ç½®
ZHIPU_API_KEY = "0f4ae0b90dff44389836ecf634297560.c1eOL2jdMckW1bfO"

# åˆå§‹åŒ– LLM å’Œ Embedding
llm = ChatZhipuAI(
    temperature=0.95,
    model="glm-4",
    api_key=ZHIPU_API_KEY,
)

embedding_model = ZhipuAIEmbeddings(
    model="embedding-3",
    api_key=ZHIPU_API_KEY
)

# æ–‡æ¡£è·¯å¾„
DATA_DIR = "data"  # è¯·ç¡®ä¿æ­¤æ–‡ä»¶å¤¹å­˜åœ¨å¹¶åŒ…å« .docx æ–‡ä»¶
CHROMA_PATH = "chroma_db"


# =======================
# 2. åŠ è½½å¹¶åˆ†å‰²æ–‡æ¡£
# =======================

# def load_documents():
#     """åŠ è½½ data/ ç›®å½•ä¸‹çš„æ‰€æœ‰ .docx æ–‡ä»¶"""
#     # loader = DirectoryLoader(DATA_DIR, glob="**/*.docx", loader_cls=Docx2txtLoader, loader_kwargs={"autodetect_encoding": "True"})
#     loader = Docx2txtLoader()
#     docs = loader.load()
#     return docs
def load_documents():
    """åŠ è½½ data/ ç›®å½•ä¸‹çš„æ‰€æœ‰ .docx æ–‡ä»¶"""
    docs = []
    for root, _, files in os.walk(DATA_DIR):
        for file in files:
            if file.endswith(".docx"):
                file_path = os.path.join(root, file)
                loader = Docx2txtLoader(file_path)
                docs.extend(loader.load())
    return docs


def split_documents(documents):
    """åˆ†å‰²æ–‡æ¡£"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    return text_splitter.split_documents(documents)


# =======================
# 3. æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆRAGï¼‰
# =======================

def get_or_create_vectorstore():
    if os.path.exists(CHROMA_PATH):
        print("åŠ è½½å·²æœ‰çš„å‘é‡æ•°æ®åº“...")
        vectorstore = Chroma(
            persist_directory=CHROMA_PATH,
            embedding_function=embedding_model
        )
    else:
        print("æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
        docs = load_documents()
        chunks = split_documents(docs)
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embedding_model,
            persist_directory=CHROMA_PATH
        )
        vectorstore.persist()
        print(f"å·²å°† {len(chunks)} ä¸ªæ–‡æœ¬å—å­˜å…¥å‘é‡åº“ã€‚")
    return vectorstore


# =======================
# 4. æŸ¥è¯¢é‡å†™ï¼ˆQuery Rewritingï¼‰
# =======================

def rewrite_query(original_query: str) -> str:
    """ä½¿ç”¨ LLM é‡å†™ç”¨æˆ·æŸ¥è¯¢ï¼Œå¢å¼ºè¯­ä¹‰"""
    rewrite_prompt = PromptTemplate.from_template(
        """
    ä½ æ˜¯ä¸€ä¸ªæ•™å­¦è¾…åŠ©æŸ¥è¯¢é‡å†™åŠ©æ‰‹ï¼Œè´Ÿè´£å¸®åŠ©æ•™å¸ˆæˆ–å­¦ç”Ÿå°†åŸå§‹é—®é¢˜ä¼˜åŒ–ä¸ºæ›´é€‚åˆè¯¾å ‚çŸ¥è¯†æ£€ç´¢ã€ä¹ é¢˜è®²è¯„å’Œå­¦ä¹ åæ€çš„æé—®å½¢å¼ã€‚

    è¯·åŸºäºä»¥ä¸‹åŸåˆ™é‡å†™é—®é¢˜ï¼š
    1. ä¿æŒåŸæ„ï¼Œä¸æ”¹å˜æé—®çš„æ ¸å¿ƒæ„å›¾ï¼›
    2. æå‡è¡¨è¾¾çš„æ¸…æ™°åº¦ä¸å­¦æœ¯æ€§ï¼Œé€‚åˆç”¨äºè¯¾ä»¶ã€æ•™æ¡ˆã€é”™é¢˜æœ¬æˆ–å­¦ä¹ èµ„æ–™çš„æ£€ç´¢ï¼›
    3. æ˜¾å¼çªå‡ºä»¥ä¸‹ä¸€ä¸ªæˆ–å¤šä¸ªç»´åº¦ï¼ˆå¦‚é€‚ç”¨ï¼‰ï¼š
       - ã€é‡ç‚¹çŸ¥è¯†ã€‘ï¼šæ¶‰åŠçš„æ ¸å¿ƒæ¦‚å¿µã€è¯¾ç¨‹æ ‡å‡†è¦æ±‚çš„å…³é”®èƒ½åŠ›ï¼›
       - ã€æ˜“é”™ç‚¹ã€‘ï¼šå­¦ç”Ÿå¸¸çŠ¯çš„è¯¯è§£ã€æ··æ·†æ¦‚å¿µæˆ–é€»è¾‘æ¼æ´ï¼›
       - ã€åšé¢˜æŠ€å·§ã€‘ï¼šè§£é¢˜çš„å…¸å‹æ–¹æ³•ã€ç­–ç•¥æˆ–æ€ç»´è·¯å¾„ï¼ˆå¦‚åˆ†ç±»è®¨è®ºã€æ•°å½¢ç»“åˆç­‰ï¼‰ï¼›
       - ã€è§„å¾‹æ€»ç»“ã€‘ï¼šå¯æ¨å¹¿çš„ç»“è®ºã€æ¨¡å‹æˆ–è§£é¢˜é€šæ³•ï¼ˆå¦‚â€œæ»‘åŠ¨å˜é˜»å™¨åˆ†å‹ç‰¹æ€§â€â€œä¸‰è§’å‡½æ•°å‘¨æœŸè§„å¾‹â€ï¼‰ï¼›
    4. é€‚å½“æ‰©å±•å…³é”®è¯æˆ–å­¦ç§‘æœ¯è¯­ï¼Œå¢å¼ºä¸æ•™å­¦èµ„æºçš„åŒ¹é…åº¦ï¼›
    5. è‹¥é—®é¢˜æ¨¡ç³Šï¼Œæ¨æµ‹å…¶å¯èƒ½æŒ‡å‘çš„çŸ¥è¯†æ¨¡å—ï¼Œå¹¶åˆç†å…·è±¡åŒ–ä¸ºå…¸å‹é—®é¢˜å½¢æ€ï¼›
    6. é¿å…å£è¯­åŒ–è¡¨è¾¾ï¼Œä½¿ç”¨è§„èŒƒçš„å­¦ç§‘è¯­è¨€å’Œæ•™å­¦è¡¨è¿°ã€‚

    åŸé—®é¢˜ï¼š{query}

    è¯·è¾“å‡ºé‡å†™åçš„é—®é¢˜ï¼Œä»…è¾“å‡ºé‡å†™ç»“æœï¼Œä¸è¦æ·»åŠ è§£é‡Šï¼š
            """
    )
    rewrite_chain = rewrite_prompt | llm | StrOutputParser()
    rewritten = rewrite_chain.invoke({"query": original_query})
    return rewritten.strip()


# =======================
# 5. æ£€ç´¢é‡æ’ï¼ˆRe-ranking via LLMï¼‰
# =======================

def create_compression_retriever(vectorstore):
    """åˆ›å»ºå¸¦é‡æ’åºçš„å‹ç¼©æ£€ç´¢å™¨"""
    compressor = LLMChainExtractor.from_llm(llm)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=retriever
    )
    return compression_retriever


# =======================
# 6. çŸ¥è¯†å›¾è°±æ„å»ºï¼ˆè½»é‡çº§ï¼‰
# =======================

# def extract_triples(text: str) -> List[Tuple[str, str, str]]:
#     """
#     ä»æ–‡æœ¬ä¸­ç®€å•æå–ä¸‰å…ƒç»„ï¼ˆå®ä½“-å…³ç³»-å®ä½“ï¼‰
#     å®é™…é¡¹ç›®ä¸­å¯ç”¨ NLP æ¨¡å‹ï¼ˆå¦‚ spaCyã€LTPï¼‰æå‡æ•ˆæœ
#     """
#     # ç®€åŒ–è§„åˆ™ï¼šåŒ¹é… "A æ˜¯ B"ã€"A åŒ…æ‹¬ B"ã€"A å¯¼è‡´ B" ç­‰æ¨¡å¼
#     patterns = [
#         (r"(.+?)æ˜¯(.+?)", "æ˜¯"),
#         (r"(.+?)åŒ…æ‹¬(.+?)", "åŒ…æ‹¬"),
#         (r"(.+?)å±äº(.+?)", "å±äº"),
#         (r"(.+?)å¯¼è‡´(.+?)", "å¯¼è‡´"),
#         (r"(.+?)å½±å“(.+?)", "å½±å“"),
#         (r"(.+?)ç»„æˆ(.+?)", "ç»„æˆ"),
#     ]
#     triples = []
#     for pattern, rel in patterns:
#         matches = re.findall(pattern, text)
#         for m in matches:
#             if isinstance(m, tuple):
#                 subj, obj = m[0].strip(), m[1].strip()
#             else:
#                 subj, obj = "", m.strip()
#             if len(subj) > 1 and len(obj) > 1:
#                 triples.append((subj, rel, obj))
#     return triples

import jieba  # ç”¨äºä¸­æ–‡åˆ†è¯ï¼Œæå‡å®ä½“è¾¹ç•Œè¯†åˆ«


def extract_triples(text: str) -> List[Tuple[str, str, str]]:
    """
    ä¼˜åŒ–ç‰ˆä¸‰å…ƒç»„æŠ½å–ï¼ˆè½»é‡è§„åˆ™ + åˆ†è¯ + å¸¸è§å¥å¼ï¼‰
    """
    if not text.strip():
        return []
    
    # åˆ†å¥
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ;\n]", text)
    triples = []
    
    # æ‰©å±•æ¨¡å¼ï¼šæ”¯æŒæ›´å¤šå¥å¼ï¼Œä½¿ç”¨æ•è·ç»„
    patterns = [
        # æ ¸å¿ƒæ¨¡å¼
        (r"(.+?)æ˜¯(.+?)[çš„]*(?:æ¦‚å¿µ|ç³»ç»Ÿ|æ–¹æ³•|æŠ€æœ¯|æ¨¡å‹|ç®—æ³•|åŸç†)", "æ˜¯"),
        (r"(.+?)æ˜¯(.+?)", "æ˜¯"),  # é€šç”¨æ˜¯
        (r"(.+?)åŒ…æ‹¬(.+?)(?:ã€(.+?))*", "åŒ…æ‹¬"),
        (r"(.+?)åŒ…å«(.+?)(?:ã€(.+?))*", "åŒ…å«"),
        (r"(.+?)å±äº(.+?)", "å±äº"),
        (r"(.+?)å¯¼è‡´(.+?)", "å¯¼è‡´"),
        (r"(.+?)å½±å“(.+?)", "å½±å“"),
        (r"(.+?)ç»„æˆ(.+?)", "ç»„æˆ"),
        (r"(.+?)ç”±(.+?)ç»„æˆ", "ç”±...ç»„æˆ"),  # åå‘
        (r"(.+?)ç”¨äº(.+?)", "ç”¨äº"),
        (r"(.+?)å®ç°(.+?)", "å®ç°"),
        (r"(.+?)åŸºäº(.+?)", "åŸºäº"),
        (r"(.+?)åˆ†ä¸º(.+?)(?:ã€(.+?))*", "åˆ†ä¸º"),
    ]
    
    for sent in sentences:
        sent = re.sub(r"[\sï¼ˆï¼‰\(\)]+", "", sent.strip())  # å»ç©ºæ ¼å’Œæ‹¬å·
        if len(sent) < 4:
            continue
        
        for pattern, rel in patterns:
            matches = re.findall(pattern, sent)
            for m in matches:
                if isinstance(m, tuple):
                    # å¤„ç†å¤šæ•è·ç»„ï¼Œå¦‚â€œåŒ…æ‹¬ Aã€Bã€Câ€
                    parts = [part.strip() for part in m if part.strip()]
                    if len(parts) < 2:
                        continue
                    subj = parts[0]
                    objects = parts[1:]
                else:
                    subj, obj = "", m.strip()
                    objects = [obj]
                
                # å¯¹æ¯ä¸ª object ç”Ÿæˆä¸‰å…ƒç»„
                for obj in objects:
                    if len(subj) > 1 and len(obj) > 1:
                        # ä½¿ç”¨ jieba ç®€å•åˆ†è¯ï¼Œé¿å…â€œæ•°å­¦ä¸­çš„åŸºæœ¬æ¦‚å¿µâ€è¿™ç§è¶…é•¿å®ä½“
                        subj = refine_entity(subj)
                        obj = refine_entity(obj)
                        triples.append((subj, rel, obj))
    
    return triples


def refine_entity(entity: str) -> str:
    """
    ç®€å•ä¼˜åŒ–å®ä½“ï¼šå»é™¤å†—ä½™è¯ã€åˆ‡åˆ†è¿‡é•¿å®ä½“
    """
    # å¸¸è§è¿‡æ»¤è¯
    stop_words = {"çš„", "ä¸€ç§", "ä¸€ä¸ª", "ä¸€ç±»", "æ‰€è°“", "æ‰€è°“", "åŸºæœ¬", "é‡è¦", "ä¸»è¦"}
    words = jieba.lcut(entity)
    words = [w for w in words if w not in stop_words and len(w) > 1]
    if not words:
        return entity.strip()
    # å–æ ¸å¿ƒè¯ï¼ˆå¯æ”¹è¿›ä¸º TF-IDF æˆ–å…³é”®è¯æå–ï¼‰
    return "".join(words[-2:]) if len(words) >= 2 else words[0]


# def build_knowledge_graph(chunks) -> nx.Graph:
#     """ä»æ–‡æ¡£å—æ„å»ºçŸ¥è¯†å›¾è°±"""
#     G = nx.Graph()
#     for chunk in chunks:
#         text = chunk.page_content
#         triples = extract_triples(text)
#         for subj, rel, obj in triples:
#             G.add_node(subj)
#             G.add_node(obj)
#             G.add_edge(subj, obj, relation=rel)
#     print(f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼šèŠ‚ç‚¹æ•°={G.number_of_nodes()}, è¾¹æ•°={G.number_of_edges()}")
#     return G


def build_knowledge_graph(chunks) -> nx.Graph:
    G = nx.Graph()
    for chunk in chunks:
        text = chunk.page_content
        triples = extract_triples(text)
        for subj, rel, obj in triples:
            G.add_node(subj)
            G.add_node(obj)
            # åˆå¹¶ç›¸åŒå…³ç³»
            if G.has_edge(subj, obj):
                # å¦‚æœå·²æœ‰è¾¹ï¼Œåˆå¹¶å…³ç³»ï¼ˆå¦‚â€œæ˜¯, åŒ…æ‹¬â€ï¼‰
                existing_rel = G[subj][obj].get("relation", "")
                if rel not in existing_rel:
                    G[subj][obj]["relation"] = existing_rel + ", " + rel
            else:
                G.add_edge(subj, obj, relation=rel)
    return G


def visualize_kg(G, top_k=20):
    """å¯è§†åŒ–çŸ¥è¯†å›¾è°±ï¼ˆå‰ top_k ä¸ªé‡è¦èŠ‚ç‚¹ï¼‰"""
    centrality = nx.degree_centrality(G)
    top_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:top_k]
    subgraph_nodes = [node for node, _ in top_nodes]
    subgraph = G.subgraph(subgraph_nodes)
    
    plt.figure(figsize=(12, 10))
    pos = nx.spring_layout(subgraph, k=0.5)
    nx.draw_networkx_nodes(subgraph, pos, node_size=500, node_color='skyblue')
    nx.draw_networkx_edges(subgraph, pos, edge_color='gray', arrows=True, arrowstyle='->', arrowsize=10)
    nx.draw_networkx_labels(subgraph, pos, font_size=10)
    edge_labels = nx.get_edge_attributes(subgraph, 'relation')
    nx.draw_networkx_edge_labels(subgraph, pos, edge_labels=edge_labels, font_size=8)
    plt.title("æ•™å­¦çŸ¥è¯†å›¾è°±ï¼ˆTop 20 èŠ‚ç‚¹ï¼‰")
    plt.axis('off')
    plt.tight_layout()
    plt.show()


# =======================
# 7. RAG é—®ç­”é“¾
# =======================

def create_rag_chain(retriever):
    """åˆ›å»º RAG é“¾"""
    prompt_template = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ•™å­¦åé¦ˆåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œå¹¶å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼š

1. **æç‚¼é‡ç‚¹çŸ¥è¯†ç‚¹**ï¼ˆåˆ—å‡º 3-5 æ¡ï¼Œæ¯æ¡ä¸è¶…è¿‡ 50 å­—ï¼‰
2. **ç”Ÿæˆ 3 é“ç»ƒä¹ é¢˜**ï¼ˆé€‰æ‹©é¢˜æˆ–ç®€ç­”é¢˜ï¼Œé™„å¸¦ç­”æ¡ˆï¼‰

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
---
### é‡ç‚¹çŸ¥è¯†ç‚¹ï¼š
1. ...
2. ...

### ç»ƒä¹ é¢˜ï¼š
1. é—®é¢˜...
   ç­”æ¡ˆï¼š...
2. ...
---
    """
    prompt = PromptTemplate.from_template(prompt_template)
    
    def format_docs(docs):
        return "\n\n".join([d.page_content for d in docs])
    
    rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )
    return rag_chain


# =======================
# 8. ä¸»ç¨‹åºå…¥å£
# =======================

def main():
    print("ğŸ“š æ™ºèƒ½æ•™å­¦åé¦ˆæ™ºèƒ½ä½“å¯åŠ¨ä¸­...")
    
    # åŠ è½½æ–‡æ¡£
    docs = load_documents()
    chunks = split_documents(docs)
    print(f"å…±åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£ï¼Œåˆ†å‰²ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")
    
    # å‘é‡åº“
    vectorstore = get_or_create_vectorstore()
    
    # å‹ç¼©æ£€ç´¢å™¨ï¼ˆå¸¦é‡æ’åºï¼‰
    retriever = create_compression_retriever(vectorstore)
    
    # æ„å»ºçŸ¥è¯†å›¾è°±
    print("æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±...")
    kg = build_knowledge_graph(chunks)
    visualize_kg(kg)
    
    # åˆ›å»º RAG é“¾
    rag_chain = create_rag_chain(retriever)
    
    # äº¤äº’å¾ªç¯
    print("\nğŸ” æ™ºèƒ½æ•™å­¦åŠ©æ‰‹å·²å‡†å¤‡å°±ç»ªï¼è¾“å…¥é—®é¢˜è·å–çŸ¥è¯†ç‚¹å’Œç»ƒä¹ é¢˜ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼Œ'kg' æŸ¥çœ‹çŸ¥è¯†å›¾è°±ï¼‰ï¼š")
    
    while True:
        query = input("\nğŸ“¥ ä½ çš„é—®é¢˜ï¼š").strip()
        if query.lower() == "quit":
            print("ğŸ‘‹ å†è§ï¼")
            break
        if query.lower() == "kg":
            visualize_kg(kg)
            continue
        if not query:
            continue
        
        print("ğŸ”„ æ­£åœ¨å¤„ç†...")
        
        # æŸ¥è¯¢é‡å†™
        rewritten_query = rewrite_query(query)
        print(f"ğŸ“ é‡å†™æŸ¥è¯¢ï¼š{rewritten_query}")
        
        # RAG å›ç­”
        try:
            response = rag_chain.invoke(rewritten_query)
            print("\nâœ… æ™ºèƒ½åé¦ˆï¼š")
            for chunk in response:
                print(chunk, end='', flush=True)
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥ï¼š{e}")


if __name__ == "__main__":
    main()
