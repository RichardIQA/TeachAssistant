# agent.py - æ›´æ–°éƒ¨åˆ†
import os
import re

import networkx as nx
from langchain_chroma import Chroma
from langchain_community.chat_models import ChatZhipuAI
from langchain_community.document_loaders import Docx2txtLoader
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_core.callbacks import BaseCallbackHandler, CallbackManager, StreamingStdOutCallbackHandler
from typing import Any, List, Tuple

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
import matplotlib
matplotlib.use('TkAgg')  # æˆ– 'Qt5Agg'ï¼Œå¿…é¡»åœ¨ import pyplot ä¹‹å‰

import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei']  # æ”¯æŒä¸­æ–‡
plt.rcParams['axes.unicode_minus'] = False

# è‡ªå®šä¹‰æµå¼å›è°ƒå¤„ç†å™¨ï¼ˆç”¨äºè¿”å›å‰ç«¯ï¼‰
ZHIPU_API_KEY = "0f4ae0b90dff44389836ecf634297560.c1eOL2jdMckW1bfO"

# åˆå§‹åŒ– LLM å’Œ Embedding
llm = ChatZhipuAI(
    temperature=0.95,
    model="glm-4",
    api_key=ZHIPU_API_KEY,
)

embedding_model = ZhipuAIEmbeddings(
    model="embedding-3",
    api_key=ZHIPU_API_KEY
)

# æ–‡æ¡£è·¯å¾„
DATA_DIR = "data"  # è¯·ç¡®ä¿æ­¤æ–‡ä»¶å¤¹å­˜åœ¨å¹¶åŒ…å« .docx æ–‡ä»¶
CHROMA_PATH = "chroma_db"


def rewrite_query(original_query: str) -> str:
    """ä½¿ç”¨ LLM é‡å†™ç”¨æˆ·æŸ¥è¯¢ï¼Œå¢å¼ºè¯­ä¹‰"""
    rewrite_prompt = PromptTemplate.from_template(
        """
    ä½ æ˜¯ä¸€ä¸ªæ•™å­¦è¾…åŠ©æŸ¥è¯¢é‡å†™åŠ©æ‰‹ï¼Œè´Ÿè´£å¸®åŠ©æ•™å¸ˆæˆ–å­¦ç”Ÿå°†åŸå§‹é—®é¢˜ä¼˜åŒ–ä¸ºæ›´é€‚åˆè¯¾å ‚çŸ¥è¯†æ£€ç´¢ã€ä¹ é¢˜è®²è¯„å’Œå­¦ä¹ åæ€çš„æé—®å½¢å¼ã€‚

    è¯·åŸºäºä»¥ä¸‹åŸåˆ™é‡å†™é—®é¢˜ï¼š
    1. ä¿æŒåŸæ„ï¼Œä¸æ”¹å˜æé—®çš„æ ¸å¿ƒæ„å›¾ï¼›
    2. æå‡è¡¨è¾¾çš„æ¸…æ™°åº¦ä¸å­¦æœ¯æ€§ï¼Œé€‚åˆç”¨äºè¯¾ä»¶ã€æ•™æ¡ˆã€é”™é¢˜æœ¬æˆ–å­¦ä¹ èµ„æ–™çš„æ£€ç´¢ï¼›
    3. æ˜¾å¼çªå‡ºä»¥ä¸‹ä¸€ä¸ªæˆ–å¤šä¸ªç»´åº¦ï¼ˆå¦‚é€‚ç”¨ï¼‰ï¼š
       - ã€é‡ç‚¹çŸ¥è¯†ã€‘ï¼šæ¶‰åŠçš„æ ¸å¿ƒæ¦‚å¿µã€è¯¾ç¨‹æ ‡å‡†è¦æ±‚çš„å…³é”®èƒ½åŠ›ï¼›
       - ã€æ˜“é”™ç‚¹ã€‘ï¼šå­¦ç”Ÿå¸¸çŠ¯çš„è¯¯è§£ã€æ··æ·†æ¦‚å¿µæˆ–é€»è¾‘æ¼æ´ï¼›
       - ã€åšé¢˜æŠ€å·§ã€‘ï¼šè§£é¢˜çš„å…¸å‹æ–¹æ³•ã€ç­–ç•¥æˆ–æ€ç»´è·¯å¾„ï¼ˆå¦‚åˆ†ç±»è®¨è®ºã€æ•°å½¢ç»“åˆç­‰ï¼‰ï¼›
       - ã€è§„å¾‹æ€»ç»“ã€‘ï¼šå¯æ¨å¹¿çš„ç»“è®ºã€æ¨¡å‹æˆ–è§£é¢˜é€šæ³•ï¼ˆå¦‚â€œæ»‘åŠ¨å˜é˜»å™¨åˆ†å‹ç‰¹æ€§â€â€œä¸‰è§’å‡½æ•°å‘¨æœŸè§„å¾‹â€ï¼‰ï¼›
    4. é€‚å½“æ‰©å±•å…³é”®è¯æˆ–å­¦ç§‘æœ¯è¯­ï¼Œå¢å¼ºä¸æ•™å­¦èµ„æºçš„åŒ¹é…åº¦ï¼›
    5. è‹¥é—®é¢˜æ¨¡ç³Šï¼Œæ¨æµ‹å…¶å¯èƒ½æŒ‡å‘çš„çŸ¥è¯†æ¨¡å—ï¼Œå¹¶åˆç†å…·è±¡åŒ–ä¸ºå…¸å‹é—®é¢˜å½¢æ€ï¼›
    6. é¿å…å£è¯­åŒ–è¡¨è¾¾ï¼Œä½¿ç”¨è§„èŒƒçš„å­¦ç§‘è¯­è¨€å’Œæ•™å­¦è¡¨è¿°ã€‚

    åŸé—®é¢˜ï¼š{query}

    è¯·è¾“å‡ºé‡å†™åçš„é—®é¢˜ï¼Œä»…è¾“å‡ºé‡å†™ç»“æœï¼Œä¸è¦æ·»åŠ è§£é‡Šï¼š
            """
    )
    rewrite_chain = rewrite_prompt | llm | StrOutputParser()
    rewritten = rewrite_chain.invoke({"query": original_query})
    return rewritten.strip()


class StreamCallbackHandler(BaseCallbackHandler):
    def __init__(self, on_token):
        self.on_token = on_token
    
    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        self.on_token(token)


# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨ç»„ä»¶ï¼ˆé¿å…é‡å¤åˆå§‹åŒ–ï¼‰
_cached = {}


def init_agent():
    if "initialized" in _cached:
        return _cached
    
    print("ğŸ§  æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½æ•™å­¦ç³»ç»Ÿ...")
    
    # ===== åŸæœ‰åˆå§‹åŒ–é€»è¾‘ =====
    from langchain_community.document_loaders import DirectoryLoader
    
    embedding_model = ZhipuAIEmbeddings(
        model="embedding-3",
        api_key="0f4ae0b90dff44389836ecf634297560.c1eOL2jdMckW1bfO"
    )
    
    DATA_DIR = "data"
    CHROMA_PATH = "chroma_db"
    
    def load_documents():
        """åŠ è½½ data/ ç›®å½•ä¸‹çš„æ‰€æœ‰ .docx æ–‡ä»¶"""
        docs = []
        for root, _, files in os.walk(DATA_DIR):
            for file in files:
                if file.endswith(".docx"):
                    file_path = os.path.join(root, file)
                    loader = Docx2txtLoader(file_path)
                    docs.extend(loader.load())
        return docs
    
    def split_documents(docs):
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        return splitter.split_documents(docs)
    
    docs = load_documents()
    chunks = split_documents(docs)
    
    if not os.path.exists(CHROMA_PATH):
        vectorstore = Chroma.from_documents(chunks, embedding_model, persist_directory=CHROMA_PATH)
        vectorstore.persist()
    else:
        vectorstore = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_model)
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
    
    # LLM æ”¯æŒæµå¼è¾“å‡º
    llm = ChatZhipuAI(
        temperature=0.95,
        model="glm-4",
        api_key="0f4ae0b90dff44389836ecf634297560.c1eOL2jdMckW1bfO",
        streaming=True,
        callbacks=CallbackManager([StreamingStdOutCallbackHandler()])  # æ§åˆ¶å°æµå¼
    )
    
    # æå–ä¸Šä¸‹æ–‡å‡½æ•°
    def format_docs(docs):
        return "\n\n".join(d.page_content for d in docs)
    
    # ä¸åœ¨æ­¤å¤„ç»‘å®šå®Œæ•´ chainï¼Œç•™å¾…æµå¼è°ƒç”¨
    _cached.update({
        "retriever": retriever,
        "llm": llm,
        "chunks": chunks,
        "format_docs": format_docs,
        "initialized": True
    })
    return _cached


import jieba  # ç”¨äºä¸­æ–‡åˆ†è¯ï¼Œæå‡å®ä½“è¾¹ç•Œè¯†åˆ«


def extract_triples(text: str) -> List[Tuple[str, str, str]]:
    """
    ä¼˜åŒ–ç‰ˆä¸‰å…ƒç»„æŠ½å–ï¼ˆè½»é‡è§„åˆ™ + åˆ†è¯ + å¸¸è§å¥å¼ï¼‰
    """
    if not text.strip():
        return []
    
    # åˆ†å¥
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ;\n]", text)
    triples = []
    
    # æ‰©å±•æ¨¡å¼ï¼šæ”¯æŒæ›´å¤šå¥å¼ï¼Œä½¿ç”¨æ•è·ç»„
    patterns = [
        # æ ¸å¿ƒæ¨¡å¼
        (r"(.+?)æ˜¯(.+?)[çš„]*(?:æ¦‚å¿µ|ç³»ç»Ÿ|æ–¹æ³•|æŠ€æœ¯|æ¨¡å‹|ç®—æ³•|åŸç†)", "æ˜¯"),
        (r"(.+?)æ˜¯(.+?)", "æ˜¯"),  # é€šç”¨æ˜¯
        (r"(.+?)åŒ…æ‹¬(.+?)(?:ã€(.+?))*", "åŒ…æ‹¬"),
        (r"(.+?)åŒ…å«(.+?)(?:ã€(.+?))*", "åŒ…å«"),
        (r"(.+?)å±äº(.+?)", "å±äº"),
        (r"(.+?)å¯¼è‡´(.+?)", "å¯¼è‡´"),
        (r"(.+?)å½±å“(.+?)", "å½±å“"),
        (r"(.+?)ç»„æˆ(.+?)", "ç»„æˆ"),
        (r"(.+?)ç”±(.+?)ç»„æˆ", "ç”±...ç»„æˆ"),  # åå‘
        (r"(.+?)ç”¨äº(.+?)", "ç”¨äº"),
        (r"(.+?)å®ç°(.+?)", "å®ç°"),
        (r"(.+?)åŸºäº(.+?)", "åŸºäº"),
        (r"(.+?)åˆ†ä¸º(.+?)(?:ã€(.+?))*", "åˆ†ä¸º"),
    ]
    
    for sent in sentences:
        sent = re.sub(r"[\sï¼ˆï¼‰\(\)]+", "", sent.strip())  # å»ç©ºæ ¼å’Œæ‹¬å·
        if len(sent) < 4:
            continue
        
        for pattern, rel in patterns:
            matches = re.findall(pattern, sent)
            for m in matches:
                if isinstance(m, tuple):
                    # å¤„ç†å¤šæ•è·ç»„ï¼Œå¦‚â€œåŒ…æ‹¬ Aã€Bã€Câ€
                    parts = [part.strip() for part in m if part.strip()]
                    if len(parts) < 2:
                        continue
                    subj = parts[0]
                    objects = parts[1:]
                else:
                    subj, obj = "", m.strip()
                    objects = [obj]
                
                # å¯¹æ¯ä¸ª object ç”Ÿæˆä¸‰å…ƒç»„
                for obj in objects:
                    if len(subj) > 1 and len(obj) > 1:
                        # ä½¿ç”¨ jieba ç®€å•åˆ†è¯ï¼Œé¿å…â€œæ•°å­¦ä¸­çš„åŸºæœ¬æ¦‚å¿µâ€è¿™ç§è¶…é•¿å®ä½“
                        subj = refine_entity(subj)
                        obj = refine_entity(obj)
                        triples.append((subj, rel, obj))
    
    return triples


def refine_entity(entity: str) -> str:
    """
    ç®€å•ä¼˜åŒ–å®ä½“ï¼šå»é™¤å†—ä½™è¯ã€åˆ‡åˆ†è¿‡é•¿å®ä½“
    """
    # å¸¸è§è¿‡æ»¤è¯
    stop_words = {"çš„", "ä¸€ç§", "ä¸€ä¸ª", "ä¸€ç±»", "æ‰€è°“", "æ‰€è°“", "åŸºæœ¬", "é‡è¦", "ä¸»è¦"}
    words = jieba.lcut(entity)
    words = [w for w in words if w not in stop_words and len(w) > 1]
    if not words:
        return entity.strip()
    # å–æ ¸å¿ƒè¯ï¼ˆå¯æ”¹è¿›ä¸º TF-IDF æˆ–å…³é”®è¯æå–ï¼‰
    return "".join(words[-2:]) if len(words) >= 2 else words[0]

def build_knowledge_graph(chunks) -> nx.Graph:
    """ä»æ–‡æ¡£å—æ„å»ºçŸ¥è¯†å›¾è°±"""
    G = nx.Graph()
    for chunk in chunks:
        text = chunk.page_content
        triples = extract_triples(text)
        for subj, rel, obj in triples:
            G.add_node(subj)
            G.add_node(obj)
            G.add_edge(subj, obj, relation=rel)
    print(f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼šèŠ‚ç‚¹æ•°={G.number_of_nodes()}, è¾¹æ•°={G.number_of_edges()}")
    return G


def visualize_kg(G, top_k=20):
    """å¯è§†åŒ–çŸ¥è¯†å›¾è°±ï¼ˆå‰ top_k ä¸ªé‡è¦èŠ‚ç‚¹ï¼‰"""
    centrality = nx.degree_centrality(G)
    top_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:top_k]
    subgraph_nodes = [node for node, _ in top_nodes]
    subgraph = G.subgraph(subgraph_nodes)
    
    plt.figure(figsize=(12, 10))
    pos = nx.spring_layout(subgraph, k=0.5)
    nx.draw_networkx_nodes(subgraph, pos, node_size=500, node_color='skyblue')
    nx.draw_networkx_edges(subgraph, pos, edge_color='gray', arrowstyle='->', arrowsize=10)
    nx.draw_networkx_labels(subgraph, pos, font_size=10)
    edge_labels = nx.get_edge_attributes(subgraph, 'relation')
    nx.draw_networkx_edge_labels(subgraph, pos, edge_labels=edge_labels, font_size=8)
    plt.title("æ•™å­¦çŸ¥è¯†å›¾è°±ï¼ˆTop 20 èŠ‚ç‚¹ï¼‰")
    plt.axis('off')
    plt.tight_layout()
    plt.show()


# æµ‹è¯•ï¼ˆä¸´æ—¶ï¼‰
if __name__ == "__main__":
    init_agent()
    print(rewrite_query("ç‰›é¡¿å®šå¾‹æ˜¯å•¥ï¼Ÿ"))
